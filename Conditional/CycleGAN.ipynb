{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"CycleGAN.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPy5Z0+8o+9pDJwXa9OfUbH"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"emTAm7LRc9hP"},"outputs":[],"source":["import tensorflow as tf\n","from tensorflow import keras\n","import time\n","import numpy as np\n","\n","# build CycleGAN\n","class CycleGAN(keras.Model):\n","    def __init__(self, lambda_, img_shape, model_type,use_identity=False):\n","        super().__init__()\n","        self.lambda_ = lambda_\n","        self.img_shape = img_shape\n","        self.use_identity = use_identity\n","\n","        self.g12 = self._get_generator(\"g12\",model_type)\n","        self.g21 = self._get_generator(\"g21\",model_type)\n","        self.d12 = self._get_discriminator(\"d12\")\n","        self.d21 = self._get_discriminator(\"d21\")\n","\n","        self.opt_G = keras.optimizers.Adam(0.0002, beta_1=0.5)\n","        self.opt_D = keras.optimizers.Adam(0.0002, beta_1=0.5)\n","        self.loss_bool = keras.losses.BinaryCrossentropy(from_logits=True)\n","        self.loss_img = keras.losses.MeanAbsoluteError()  # a better result when using mse\n","\n","        #summary\n","        # self.g12.summary\n","        # self.d12.summary\n","        \n","    def d_loss_wasserstein(self,real_logits,fake_logits):\n","        \n","        d_loss=tf.reduce_mean(fake_logits)-tf.reduce_mean(real_logits)\n","\n","        return d_loss\n","\n","    def g_loss_wasserstein(self,fake_logits):\n","\n","        g_loss=-tf.reduce_mean(fake_logits)\n","\n","        return g_loss\n","\n","    def wasserstein_gradient_penalty(self,x,x_fake,y,discriminator):\n","\n","        # temp_shape = [x.shape[0]]+[1 for _ in  range(len(x.shape)-1)]\n","\n","        epsilon = tf.random.uniform([], 0.0, 1.0)\n","        x_hat = epsilon*x + (1 - epsilon) * x_fake\n","        \n","        with tf.GradientTape() as t:\n","          t.watch(x_hat)\n","          d_hat = discriminator([x_hat,y],training=False)\n","        gradients = t.gradient(d_hat, x_hat)\n","        slopes = tf.sqrt(tf.reduce_sum(tf.square(gradients)))\n","        gradient_penalty = 1 * tf.reduce_mean((slopes - 1.0) ** 2)\n","\n","        return gradient_penalty\n","\n","    def _get_generator(self, name,model_type):\n","        if model_type=='sem':\n","          model = condition_mnist_uni_img2img_sem((28, 28, 1),name=name)\n","          print('Load ResNet with SEM module...')\n","        else:\n","\n","          model = condition_mnist_uni_img2img((28, 28, 1),name=name)\n","          print('Load ResNet module...')\n","\n","        return model\n","\n","    def _get_discriminator(self, name):\n","        model = condition_mnist_uni_disc_cnn((28,28,1),name=name)\n","        return model\n","\n","    def cycle_loss(self, real_img1,real_y1, real_img2,real_y2):\n","        fake2, fake1 = self.g12([real_img1,real_y2]), self.g21([real_img2,real_y1])\n","\n","        loss1 = self.loss_img(real_img1, self.g21([fake2,real_y1]))\n","        loss2 = self.loss_img(real_img2, self.g12([fake1,real_y2]))\n","\n","        cycle_loss = loss1 + loss2\n","\n","        return cycle_loss, fake2, fake1\n","\n","    def train_g(self, img1,y1, img2,y2,loss_type):\n","        with tf.GradientTape() as tape:\n","            cycle_loss, fake2, fake1 = self.cycle_loss(img1,y1, img2,y2)\n","            # img1\n","            pred2 = self.d12([fake2,y2])\n","            # img2\n","            pred1 = self.d21([fake1,y1])\n","\n","            if loss_type=='wd' or loss_type=='wd-sem':\n","              # print('Load Wasserstein distance as loss...')\n","              #wd\n","              d_loss12=self.g_loss_wasserstein(pred2)\n","              # wd\n","              d_loss21=self.g_loss_wasserstein(pred1)\n","\n","            else:\n","              # print('Load sliced Wasserstein distance as loss...')\n","              #swd\n","              r_y2 = self.d12([img2,y2])\n","              d_loss12 = sw_loss(r_y2, pred2,num_projections=32,batch_size=32)\n","\n","              #swd\n","              r_y1 = self.d21([img1,y1])\n","              d_loss21=sw_loss(r_y1, pred1,num_projections=32,batch_size=32)\n","\n","            #kl\n","            # d_loss12 = self.loss_bool(tf.ones_like(pred2), pred2)\n","            #kl\n","            # d_loss21 = self.loss_bool(tf.ones_like(pred1), pred1)\n","\n","            loss12 = d_loss12\n","            loss21 = d_loss21           \n","\n","            loss = loss12 + loss21+ self.lambda_ * cycle_loss\n","        var = self.g12.trainable_variables + self.g21.trainable_variables\n","        grads = tape.gradient(loss, var)\n","        self.opt_G.apply_gradients(zip(grads, var))\n","\n","        return d_loss12 + d_loss21, cycle_loss\n","\n","    def train_d(self, img1,y1_, img2,y2_):\n","        length = len(img1)  # length of img1=length of img2\n","\n","        with tf.GradientTape() as d_tape:\n","            fake2, fake1 = self.g12([img1,y2_]), self.g21([img2,y1_])\n","\n","            y_real = tf.ones((length, 1), tf.float32)\n","            y_fake = tf.zeros((length, 1), tf.float32)\n","\n","            # adversarial_1\n","            y2 = self.d12([img2,y2_])\n","            pred2 = self.d12([fake2,y2_])\n","            #loss2_real = self.loss_bool(y_real, y2)\n","            #loss2_fake = self.loss_bool(y_fake, pred2)\n","\n","            #loss_12 = loss2_real + loss2_fake\n","\n","            loss_12=self.d_loss_wasserstein(y2,pred2)\n","            loss_12+=self.wasserstein_gradient_penalty(x=img2,x_fake=fake2,y=y2_,\n","                                                  discriminator=self.d12)\n","            # adversarial_2\n","            y1 = self.d21([img1,y1_])\n","            pred1 = self.d21([fake1,y1_])\n","\n","            #loss1_real = self.loss_bool(y_real, y1)\n","            #loss1_fake = self.loss_bool(y_fake, pred1)\n","\n","            #loss_21 = loss1_real + loss1_fake\n","            loss_21=self.d_loss_wasserstein(y1,pred1)\n","            loss_21+=self.wasserstein_gradient_penalty(x=img1,x_fake=fake1,y=y1_,\n","                                                  discriminator=self.d21)\n","\n","            # total adversarial loss\n","            dis_loss = loss_12 + loss_21\n","\n","        var = self.d12.trainable_variables + self.d21.trainable_variables\n","        dis_grads = d_tape.gradient(dis_loss, var)\n","        self.opt_D.apply_gradients(zip(dis_grads, var))\n","        return dis_loss\n","\n","    def train_on_step(self, img1,y1, img2,y2,loss_type):\n","        # for _ in range(5):\n","        d_loss = self.train_d(img1,y1,img2,y2)\n","        g_loss, cyc_loss = self.train_g(img1,y1, img2,y2,loss_type)\n","\n","        return g_loss, cyc_loss, d_loss\n","\n","def train(seed,loss_type,gan, x0, y0, ds_x, ds_y, test6, testy_6, test9, testy_9, step, batch_size):\n","    loss_G=[]\n","    loss_D=[]\n","    loss_CYC=[]\n","    s_value=[]\n","    r_value=[]\n","\n","    general='drive/Shared drives/Ziqiang/Mnist_condi/For_github/'+loss_type+'/'+seed\n","    dir_=general+'/visual/'\n","    dir_loss=general+'/loss/'\n","    dir_model=general+'/models/'\n","    dir_utils=general+'/others/'\n","\n","    os.makedirs(dir_,exist_ok=True)\n","    os.makedirs(dir_loss,exist_ok=True)\n","    os.makedirs(dir_utils,exist_ok=True)\n","    t0 = time.time()\n","\n","    rate=0\n","    for t in range(step):\n","        idx6 = np.random.randint(0, len(x0), batch_size)\n","        img6 = tf.gather(x0, idx6)\n","        y6 = tf.gather(y0, idx6)\n","\n","        idx9 = np.random.randint(0, len(ds_x), batch_size)\n","        img9 = tf.gather(ds_x, idx9)\n","        y9 = tf.gather(ds_y, idx9)\n","\n","        g_loss, d_loss, cyc_loss = gan.train_on_step(img6, y6, img9, y9,loss_type)\n","\n","        current_score,x_eval,y_eval,img,imgs=save_gan(gan,t,img6=test6,img9=test9,\n","                                             y6=testy_6,y9=testy_9,)\n","        \n","        loss_G.append(g_loss.numpy())\n","        loss_D.append(d_loss.numpy())\n","        loss_CYC.append(cyc_loss.numpy())\n","\n","        if t==0:\n","          score0=0.9\n","          print('initial:',score0)\n","\n","          s_value.append([score0,t])\n","          r_value.append([0,t])\n","\n","        \n","        else:\n","          score=current_score\n","          s_value.append([score,t])\n","          current_rate=1-score/score0\n","\n","          if t%200==0:\n","            r_value.append([current_rate,t])\n","            print('Rate:',current_rate,'at step:',t,'Cost: gen',g_loss.numpy(), 'dis',d_loss.numpy(),'cyc', cyc_loss.numpy())\n","            rate=current_rate\n","            best_step=t\n","\n","            #visual\n","            path=dir_+\"{}.png\".format(best_step)\n","            t_sne(x_eval,y_eval,n_class=18,savename=path)\n","\n","            #models\n","            os.makedirs(dir_model+'model_'+str(best_step),exist_ok=True)\n","            gan.save_weights(dir_model+'model_'+str(t)+'/model.ckpt')\n","\n","            #MNIST imgae\n","            _save_img2img_gan( best_step, img, imgs,seed,loss_type)\n","\n","    \n","    #visual\n","    rate_last=1-current_score/score0\n","    r_value.append([rate_last,t])\n","    print('last rate:',rate_last)\n","    path=dir_+\"/{}.png\".format(t)\n","    t_sne(x_eval,y_eval,n_class=18,savename=path)\n","\n","    #model\n","    gan.save_weights(dir_model+'model_'+str(t)+'/model.ckpt')\n","\n","    #MNIST imgae\n","    _save_img2img_gan( ep, img, imgs,seed,loss_type)\n","\n","\n","    t1 = time.time()\n","    print('running time:',t1-t0)\n","    #loss\n","    np.savetxt(dir_loss+'loss_g.txt',np.array(loss_G))\n","    np.savetxt(dir_loss+'loss_d.txt',np.array(loss_D))\n","    np.savetxt(dir_loss+'loss_cyc.txt',np.array(loss_CYC))\n","\n","    #others\n","    np.savetxt(dir_utils+'score.txt',np.array(s_value))\n","    np.savetxt(dir_utils+'rate.txt',np.array(r_value))\n","\n","    print('running time:', t1 - t0)\n","    print('*'*20)\n","    print('Average rate is:',np.array(r_value)[1:,0].mean())\n","    print('*'*20)"]}]}
