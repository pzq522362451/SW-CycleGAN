{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"swd_tf2.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyOzk/0eVoMIrnMNcSd1w3AT"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"D2D21ffbQdlv"},"outputs":[],"source":["#SWD TF2 version\n","import tensorflow as tf\n","def sw_loss(true_distribution, \n","            generated_distribution,\n","            num_projections,batch_size):\n","  \n","  s = true_distribution.get_shape().as_list()[-1]\n","  \n","  # num_projections=140\n","  # batch_size=140\n","  theta = tf.random.normal(shape=[s, num_projections])\n","  theta = tf.nn.l2_normalize(theta, axis=0)\n","\n","  # project the samples (images). After being transposed, we have tensors\n","  # of the format: [projected_image1, projected_image2, ...].\n","  # Each row has the projections along one direction. This makes it\n","  # easier for the sorting that follows.\n","  projected_true = tf.transpose(\n","      tf.matmul(true_distribution, theta))\n","\n","  projected_fake = tf.transpose(\n","      tf.matmul(generated_distribution, theta))\n","\n","  sorted_true, true_indices = tf.nn.top_k(\n","      projected_true,\n","      batch_size)\n","\n","  sorted_fake, fake_indices = tf.nn.top_k(\n","      projected_fake,\n","      batch_size)\n","  # print(sorted_fake.shape, fake_indices.shape)\n","\n","  # For faster gradient computation, we do not use sorted_fake to compute\n","  # loss. Instead we re-order the sorted_true so that the samples from the\n","  # true distribution go to the correct sample from the fake distribution.\n","  # This is because Tensorflow did not have a GPU op for rearranging the\n","  # gradients at the time of writing this code.\n","\n","  # It is less expensive (memory-wise) to rearrange arrays in TF.\n","  # Flatten the sorted_true from [batch_size, num_projections].\n","  flat_true = tf.reshape(sorted_true, [-1])\n","\n","  # Modify the indices to reflect this transition to an array.\n","  # new index = row + index\n","  rows = np.asarray(\n","      [batch_size * np.floor(i * 1.0 / batch_size)\n","        for i in range(num_projections * batch_size)])\n","  rows = rows.astype(np.int32)\n","  flat_idx = tf.reshape(fake_indices, [-1, 1]) + np.reshape(rows, [-1, 1])\n","\n","  # The scatter operation takes care of reshaping to the rearranged matrix\n","  shape = tf.constant([batch_size * num_projections])\n","  rearranged_true = tf.reshape(\n","      tf.scatter_nd(flat_idx, flat_true, shape),\n","      [num_projections, batch_size])\n","\n","  return tf.reduce_mean(tf.square(projected_fake - rearranged_true))"]}]}